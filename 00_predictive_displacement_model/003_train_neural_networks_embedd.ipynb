{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12642d3",
   "metadata": {},
   "source": [
    "# 1. LIBRERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588422a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, Dropout, Concatenate, \n",
    "    Flatten, BatchNormalization, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score,\n",
    "    matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score, \n",
    "    log_loss, brier_score_loss\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2c6e9",
   "metadata": {},
   "source": [
    "# 2. GPU CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9cf46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"\\n✓ GPU DETECTED:\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  • GPU {i}: {gpu.name}\")\n",
    "        print(\"  • Memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\nNo GPU - using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2967342",
   "metadata": {},
   "source": [
    "# 3. CREATE FOLDER STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'db/02b_neural_networks/saved_models',\n",
    "    'db/02b_neural_networks/predictions',\n",
    "    'db/02b_neural_networks/metrics',\n",
    "    'db/02b_neural_networks/metrics/by_architecture',\n",
    "    'db/02b_neural_networks/model_data/confusion_matrices',\n",
    "    'db/02b_neural_networks/model_data/confusion_matrices/by_architecture',\n",
    "    'db/02b_neural_networks/model_data/roc_data',\n",
    "    'db/02b_neural_networks/model_data/roc_data/by_architecture',\n",
    "    'db/02b_neural_networks/model_data/learning_curves',\n",
    "    'db/02b_neural_networks/model_data/learning_curves/by_architecture',\n",
    "    'db/02b_neural_networks/model_data/architecture_comparisons',\n",
    "    'db/02b_neural_networks/model_data/hyperparameters',\n",
    "    'db/02b_neural_networks/comparative_tables'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c8813",
   "metadata": {},
   "source": [
    "# 4. LOAD CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420829a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'db/01_cleaned_data/displacement_vs_others_final.csv'\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "print(f\"\\n Data loaded: {len(df):,} records\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61c904",
   "metadata": {},
   "source": [
    "# 5. DATA PREPARATION (FULL PIPELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_vars = [# Categorical variables\n",
    "                  'ESTADO_DEPTO',                    \n",
    "                  'SEXO',            \n",
    "                  'ETNIA',           \n",
    "                  'DISCAPACIDAD',    \n",
    "                  'CICLO_VITAL', \n",
    "                  # Numeric variables\n",
    "                  'VIGENCIA',          \n",
    "                  'EVENTOS',\n",
    "                  'km_norte_sur', \n",
    "                  'km_este_oeste', \n",
    "                  'distancia_total'\n",
    "                  ]\n",
    "\n",
    "target_var = 'Desplazamiento_forzado_binaria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EVENTOS'] = pd.to_numeric(df['EVENTOS'], errors='coerce')\n",
    "df['VIGENCIA'] = pd.to_numeric(df['VIGENCIA'], errors='coerce')\n",
    "df['km_norte_sur'] = pd.to_numeric(df['km_norte_sur'], errors='coerce')\n",
    "df['km_este_oeste'] = pd.to_numeric(df['km_este_oeste'], errors='coerce')\n",
    "df['distancia_total'] = pd.to_numeric(df['distancia_total'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df[predictor_vars + [target_var]].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\nMissing values detected:\")\n",
    "    print(missing[missing > 0])\n",
    "    df = df.dropna(subset=predictor_vars + [target_var])\n",
    "    print(f\"Records after removing missing: {len(df):,}\")\n",
    "else:\n",
    "    print(\"\\nNo missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a090536",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictor_vars].copy()\n",
    "y = df[target_var].values\n",
    "\n",
    "predictor_cat_cols = ['SEXO', 'ETNIA', 'CICLO_VITAL', 'DISCAPACIDAD', 'ESTADO_DEPTO']\n",
    "predictor_num_cols = ['EVENTOS', 'VIGENCIA', 'km_norte_sur', 'km_este_oeste', 'distancia_total']\n",
    "\n",
    "print(f\"\\nDataset prepared:\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "print(f\"Categorical: {predictor_cat_cols}\")\n",
    "print(f\"Numeric: {predictor_num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = pd.Series(y).value_counts()\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Class 0 (Others): {class_counts.get(0,0):,} ({class_counts.get(0,0)/len(y)*100:.2f}%)\")\n",
    "print(f\"Class 1 (Displacement): {class_counts.get(1,0):,} ({class_counts.get(1,0)/len(y)*100:.2f}%)\")\n",
    "if class_counts.get(1,0) > 0:\n",
    "    print(f\"Imbalance ratio: {class_counts.get(0,0)/class_counts.get(1,0):.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b86aa",
   "metadata": {},
   "source": [
    "## 5.1. CATEGORICAL ENCODING FOR EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "X_cat_encoded = {}\n",
    "\n",
    "for col in predictor_cat_cols:\n",
    "    enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "    X_cat_encoded[col] = enc.fit_transform(X[[col]].astype(str)).astype('int32').flatten()\n",
    "    encoders[col] = enc\n",
    "    n_unique = len(enc.categories_[0])\n",
    "    print(f\"  • {col}: {n_unique} categories\")\n",
    "\n",
    "joblib.dump(encoders, 'db/02b_neural_networks/saved_models/categorical_encoders.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a6038",
   "metadata": {},
   "source": [
    "## 5.2. EMBEDDING DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_info = {}\n",
    "\n",
    "for col in predictor_cat_cols:\n",
    "    n_categories = len(encoders[col].categories_[0])\n",
    "    embed_dim = min(50, (n_categories + 1) // 2)\n",
    "    \n",
    "    embedding_info[col] = {\n",
    "        'n_categories': n_categories,\n",
    "        'embed_dim': embed_dim\n",
    "    }\n",
    "    print(f\"{col}: {n_categories}: {embed_dim}D\")\n",
    "\n",
    "joblib.dump(embedding_info, 'db/02b_neural_networks/saved_models/embedding_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a85524",
   "metadata": {},
   "source": [
    "## 5.3. NUMERIC SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ac7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa el mismo pipeline de escalamiento que los modelos clásicos\n",
    "numeric_report = []\n",
    "scalers = {}\n",
    "\n",
    "for col in predictor_num_cols:\n",
    "    s = X[col].dropna().astype(float)\n",
    "    n_obs = len(s)\n",
    "    n_unique = s.nunique()\n",
    "    mean, std = s.mean(), s.std()\n",
    "    minimum, maximum = s.min(), s.max()\n",
    "    skew = s.skew()\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    outliers = ((s < lower) | (s > upper)).sum()\n",
    "    outlier_ratio = outliers / n_obs if n_obs > 0 else 0\n",
    "\n",
    "    # Log transform si existe mucha asimetría\n",
    "    log_transform = (abs(skew) > 2) and (minimum >= 0)\n",
    "\n",
    "    # Selección automática del escalador\n",
    "    if outlier_ratio > 0.05:\n",
    "        scaler_choice, scaler = 'RobustScaler', RobustScaler()\n",
    "    else:\n",
    "        if (minimum >= 0) and (maximum <= 1e6):\n",
    "            scaler_choice, scaler = 'MinMaxScaler', MinMaxScaler()\n",
    "        else:\n",
    "            scaler_choice, scaler = 'StandardScaler', StandardScaler()\n",
    "\n",
    "    col_data = X[[col]].astype(float).copy()\n",
    "    if log_transform:\n",
    "        col_data = np.log1p(col_data.clip(lower=0))\n",
    "        col_data = col_data.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    scaled = scaler.fit_transform(col_data.fillna(0))\n",
    "    X[col] = scaled  # <-- Inserta en el DataFrame global\n",
    "\n",
    "    numeric_report.append({\n",
    "        'variable': col,\n",
    "        'n_obs': int(n_obs),\n",
    "        'n_unique': int(n_unique),\n",
    "        'mean': float(mean),\n",
    "        'std': float(std),\n",
    "        'min': float(minimum),\n",
    "        'max': float(maximum),\n",
    "        'skew': float(skew),\n",
    "        'outlier_ratio': float(outlier_ratio),\n",
    "        'log_transform_applied': bool(log_transform),\n",
    "        'scaler_chosen': scaler_choice\n",
    "    })\n",
    "\n",
    "    scalers[col] = scaler\n",
    "\n",
    "# Guarda escaladores\n",
    "joblib.dump(scalers, 'db/02b_neural_networks/saved_models/numeric_scalers.pkl')\n",
    "\n",
    "# Matriz para ANN\n",
    "X_num_scaled = X[predictor_num_cols].astype('float32').values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432f5b1",
   "metadata": {},
   "source": [
    "## 5.4. TRAIN/TEST SPLIT (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_all = np.arange(len(y))\n",
    "idx_train, idx_test = train_test_split(idx_all, test_size=0.30, random_state=42, stratify=y)\n",
    "\n",
    "X_train_cat = {col: X_cat_encoded[col][idx_train] for col in predictor_cat_cols}\n",
    "X_test_cat = {col: X_cat_encoded[col][idx_test] for col in predictor_cat_cols}\n",
    "\n",
    "X_train_num = X_num_scaled[idx_train]\n",
    "X_test_num = X_num_scaled[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cea958",
   "metadata": {},
   "source": [
    "## 5.5. COMBINING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[idx_train]\n",
    "y_test = y[idx_test]\n",
    "\n",
    "print(f\"Train: {len(y_train):,}\")\n",
    "print(f\"Test: {len(y_test):,}\")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {int(k): float(v) for k, v in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "print(f\"\\nClass weights: \\n0={class_weight_dict[0]:.3f}, 1={class_weight_dict[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8de3b",
   "metadata": {},
   "source": [
    "# 6. VALIDATION SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeda354",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_subtrain = np.arange(len(y_train))\n",
    "idx_subtrain_split, idx_val_split = train_test_split(\n",
    "    idx_subtrain, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "X_cat_subtrain = {col: X_train_cat[col][idx_subtrain_split] for col in predictor_cat_cols}\n",
    "X_cat_val = {col: X_train_cat[col][idx_val_split] for col in predictor_cat_cols}\n",
    "\n",
    "X_num_subtrain = X_train_num[idx_subtrain_split]\n",
    "X_num_val = X_train_num[idx_val_split]\n",
    "\n",
    "y_subtrain = y_train[idx_subtrain_split]\n",
    "y_val = y_train[idx_val_split]\n",
    "\n",
    "print(f\"  • Subtrain: {len(y_subtrain):,}\")\n",
    "print(f\"  • Validation: {len(y_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b976142",
   "metadata": {},
   "source": [
    "# 7. FOCAL LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f730b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        modulating_factor = K.pow(1.0 - pt, gamma)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        cross_entropy = -K.log(pt)\n",
    "        loss = alpha_factor * modulating_factor * cross_entropy\n",
    "        return K.mean(loss)\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231f35a",
   "metadata": {},
   "source": [
    "# 8. HELPER FUNCTIONS FOR COMPREHENSIVE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nn_comprehensive_metrics(model, model_name, arch_name, \n",
    "                                       X_train_inputs, X_test_inputs,\n",
    "                                       y_train, y_test, \n",
    "                                       training_time_minutes,\n",
    "                                       batch_size=2048):\n",
    "    \n",
    "    y_prob_train = model.predict(X_train_inputs, batch_size=batch_size, verbose=0).flatten()\n",
    "    y_pred_train = (y_prob_train > 0.5).astype(int)\n",
    "    \n",
    "    y_prob_test = model.predict(X_test_inputs, batch_size=batch_size, verbose=0).flatten()\n",
    "    y_pred_test = (y_prob_test > 0.5).astype(int)\n",
    "    \n",
    "    inference_start = time.time()\n",
    "    _ = model.predict(X_test_inputs, batch_size=batch_size, verbose=0)\n",
    "    inference_time = (time.time() - inference_start) / len(y_test) * 1000\n",
    "    \n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    tn, fp, fn, tp = cm_test.ravel()\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    train_precision = precision_score(y_train, y_pred_train, zero_division=0)\n",
    "    test_precision = precision_score(y_test, y_pred_test, zero_division=0)\n",
    "    train_recall = recall_score(y_train, y_pred_train, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_pred_test, zero_division=0)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_pred_test, zero_division=0)\n",
    "    \n",
    "    train_specificity = cm_train[0, 0] / (cm_train[0, 0] + cm_train[0, 1]) if (cm_train[0, 0] + cm_train[0, 1]) > 0 else 0\n",
    "    test_specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    train_gmean = np.sqrt(train_recall * train_specificity)\n",
    "    test_gmean = np.sqrt(test_recall * test_specificity)\n",
    "    \n",
    "    train_mcc = matthews_corrcoef(y_train, y_pred_train)\n",
    "    test_mcc = matthews_corrcoef(y_test, y_pred_test)\n",
    "    \n",
    "    train_balanced_acc = balanced_accuracy_score(y_train, y_pred_train)\n",
    "    test_balanced_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_kappa = cohen_kappa_score(y_train, y_pred_train)\n",
    "    test_kappa = cohen_kappa_score(y_test, y_pred_test)\n",
    "    \n",
    "    test_roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "    test_log_loss = log_loss(y_test, y_prob_test)\n",
    "    test_brier_score = brier_score_loss(y_test, y_prob_test)\n",
    "    \n",
    "    model_size_mb = sum([np.prod(w.shape) * 4 for w in model.get_weights()]) / (1024 * 1024)\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([K.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    \n",
    "    accuracy_gap = train_accuracy - test_accuracy\n",
    "    f1_gap = train_f1 - test_f1\n",
    "    precision_gap = train_precision - test_precision\n",
    "    recall_gap = train_recall - test_recall\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Architecture': arch_name,\n",
    "        'Train_Accuracy': train_accuracy,\n",
    "        'Train_Precision': train_precision,\n",
    "        'Train_Recall': train_recall,\n",
    "        'Train_F1': train_f1,\n",
    "        'Train_Specificity': train_specificity,\n",
    "        'Train_G_Mean': train_gmean,\n",
    "        'Train_Balanced_Accuracy': train_balanced_acc,\n",
    "        'Train_MCC': train_mcc,\n",
    "        'Train_Kappa': train_kappa,\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Test_Precision': test_precision,\n",
    "        'Test_Recall': test_recall,\n",
    "        'Test_F1': test_f1,\n",
    "        'Test_Specificity': test_specificity,\n",
    "        'Test_G_Mean': test_gmean,\n",
    "        'Test_Balanced_Accuracy': test_balanced_acc,\n",
    "        'Test_MCC': test_mcc,\n",
    "        'Test_Kappa': test_kappa,\n",
    "        'Test_ROC_AUC': test_roc_auc,\n",
    "        'Test_Log_Loss': test_log_loss,\n",
    "        'Test_Brier_Score': test_brier_score,\n",
    "        'Test_True_Negatives': int(tn),\n",
    "        'Test_False_Positives': int(fp),\n",
    "        'Test_False_Negatives': int(fn),\n",
    "        'Test_True_Positives': int(tp),\n",
    "        'Test_TN_Percentage': (tn / len(y_test)) * 100,\n",
    "        'Test_FP_Percentage': (fp / len(y_test)) * 100,\n",
    "        'Test_FN_Percentage': (fn / len(y_test)) * 100,\n",
    "        'Test_TP_Percentage': (tp / len(y_test)) * 100,\n",
    "        'Accuracy_Gap_Train_Test': accuracy_gap,\n",
    "        'F1_Gap_Train_Test': f1_gap,\n",
    "        'Precision_Gap_Train_Test': precision_gap,\n",
    "        'Recall_Gap_Train_Test': recall_gap,\n",
    "        'Training_Time_Minutes': training_time_minutes,\n",
    "        'Inference_Time_ms_per_sample': inference_time,\n",
    "        'Model_Size_MB': model_size_mb,\n",
    "        'Total_Parameters': int(total_params),\n",
    "        'Trainable_Parameters': int(trainable_params),\n",
    "        'Non_Trainable_Parameters': int(non_trainable_params),\n",
    "        'Total_Train_Samples': len(y_train),\n",
    "        'Total_Test_Samples': len(y_test)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_test, y_prob_test\n",
    "\n",
    "\n",
    "def build_tf_datasets(X_cat_dict_train, X_num_train, y_train,\n",
    "                      X_cat_dict_val, X_num_val, y_val,\n",
    "                      predictor_cat_cols,\n",
    "                      batch_size=1024, shuffle_buffer=100000):\n",
    "    \n",
    "    def dicts_to_list(cat_dict, num_array):\n",
    "        arrs = [cat_dict[col].astype('int32') for col in predictor_cat_cols]\n",
    "        if (num_array is not None) and (num_array.size > 0):\n",
    "            arrs.append(num_array.astype('float32'))\n",
    "        return arrs\n",
    "\n",
    "    X_train_list = dicts_to_list(X_cat_dict_train, X_num_train)\n",
    "    X_val_list = dicts_to_list(X_cat_dict_val, X_num_val)\n",
    "\n",
    "    train_ds = Dataset.from_tensor_slices((tuple(X_train_list), y_train.astype('float32')))\n",
    "    val_ds = Dataset.from_tensor_slices((tuple(X_val_list), y_val.astype('float32')))\n",
    "\n",
    "    train_ds = train_ds.shuffle(shuffle_buffer).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d970e",
   "metadata": {},
   "source": [
    "# 9. NETWORK ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6627a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_style(embedding_info, num_numeric_feats, predictor_cat_cols,\n",
    "                       num_blocks=3, base_units=256, dropout_rate=0.3,\n",
    "                       learning_rate=1e-3):\n",
    "    inputs = []\n",
    "    embed_outputs = []\n",
    "\n",
    "    for col in predictor_cat_cols:\n",
    "        info = embedding_info[col]\n",
    "        n_cat = info['n_categories']\n",
    "        d_emb = info['embed_dim']\n",
    "\n",
    "        inp = Input(shape=(1,), name=f\"inp_{col}\")\n",
    "        emb = Embedding(input_dim=n_cat, output_dim=d_emb, name=f\"emb_{col}\")(inp)\n",
    "        flat = Flatten()(emb)\n",
    "        inputs.append(inp)\n",
    "        embed_outputs.append(flat)\n",
    "\n",
    "    if num_numeric_feats > 0:\n",
    "        num_inp = Input(shape=(num_numeric_feats,), name=\"inp_numeric\")\n",
    "        inputs.append(num_inp)\n",
    "        embed_outputs.append(num_inp)\n",
    "\n",
    "    x = Concatenate()(embed_outputs) if len(embed_outputs) > 1 else embed_outputs[0]\n",
    "\n",
    "    x = Dense(base_units, activation='relu', name='initial_dense')(x)\n",
    "    x = BatchNormalization(name='initial_bn')(x)\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = Dense(base_units, activation='relu', name=f'block_{i+1}_dense_1')(x)\n",
    "        x = BatchNormalization(name=f'block_{i+1}_bn_1')(x)\n",
    "        x = Dropout(dropout_rate, name=f'block_{i+1}_dropout_1')(x)\n",
    "        \n",
    "        x = Dense(base_units, activation='relu', name=f'block_{i+1}_dense_2')(x)\n",
    "        x = BatchNormalization(name=f'block_{i+1}_bn_2')(x)\n",
    "        \n",
    "        x = Add(name=f'block_{i+1}_add')([x, shortcut])\n",
    "        x = Dropout(dropout_rate, name=f'block_{i+1}_dropout_2')(x)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.AUC(name='auc_roc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_wide_and_deep(embedding_info, num_numeric_feats, predictor_cat_cols,\n",
    "                        deep_layers=[512, 256, 128], dropout_rate=0.3,\n",
    "                        learning_rate=1e-3):\n",
    "    inputs = []\n",
    "    embed_outputs = []\n",
    "    wide_inputs = []\n",
    "\n",
    "    for col in predictor_cat_cols:\n",
    "        info = embedding_info[col]\n",
    "        n_cat = info['n_categories']\n",
    "        d_emb = info['embed_dim']\n",
    "\n",
    "        inp = Input(shape=(1,), name=f\"inp_{col}\")\n",
    "        \n",
    "        emb = Embedding(input_dim=n_cat, output_dim=d_emb, name=f\"emb_{col}\")(inp)\n",
    "        flat = Flatten()(emb)\n",
    "        embed_outputs.append(flat)\n",
    "        \n",
    "        wide_inputs.append(inp)\n",
    "        inputs.append(inp)\n",
    "\n",
    "    if num_numeric_feats > 0:\n",
    "        num_inp = Input(shape=(num_numeric_feats,), name=\"inp_numeric\")\n",
    "        inputs.append(num_inp)\n",
    "        embed_outputs.append(num_inp)\n",
    "        wide_inputs.append(num_inp)\n",
    "\n",
    "    deep = Concatenate(name='deep_concat')(embed_outputs) if len(embed_outputs) > 1 else embed_outputs[0]\n",
    "    \n",
    "    for i, units in enumerate(deep_layers):\n",
    "        deep = Dense(units, activation='relu', name=f\"deep_dense_{i+1}\")(deep)\n",
    "        deep = BatchNormalization(name=f\"deep_bn_{i+1}\")(deep)\n",
    "        deep = Dropout(dropout_rate, name=f\"deep_dropout_{i+1}\")(deep)\n",
    "\n",
    "    wide = Concatenate(name='wide_concat')(wide_inputs) if len(wide_inputs) > 1 else wide_inputs[0]\n",
    "    wide = Flatten(name='wide_flatten')(wide)\n",
    "\n",
    "    combined = Concatenate(name='wide_deep_concat')([wide, deep])\n",
    "    output = Dense(1, activation='sigmoid', name='output')(combined)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.AUC(name='auc_roc'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482f5d4",
   "metadata": {},
   "source": [
    "# 8. DEFINING MODELS AND ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97945f8c",
   "metadata": {},
   "source": [
    "Due to the computer used for training not having a high capacity to train all models at once, a comment and uncomment strategy was implemented to train three architectures per model. To execute, the three architectures to be trained must be uncommented, and the rest must be left commented. All information is saved and will not be lost when training other models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_num_feats = X_train_num.shape[1]\n",
    "\n",
    "network_configs = {\n",
    "\n",
    "\n",
    "    # 'ResNet_Style': {\n",
    "    #     'network_type': 'resnet_style',\n",
    "    #     'architectures': [\n",
    "    #         {\n",
    "    #             'name': 'ResNet_Style_Architecture_1',\n",
    "    #             'params': {\n",
    "    #                 'num_blocks': 2,\n",
    "    #                 'base_units': 256,\n",
    "    #                 'dropout_rate': 0.3,\n",
    "    #                 'learning_rate': 1e-3,\n",
    "    #                 'batch_size': 2048,\n",
    "    #                 'epochs': 200\n",
    "    #             }\n",
    "    #         },\n",
    "    #         {\n",
    "    #             'name': 'ResNet_Style_Architecture_2',\n",
    "    #             'params': {\n",
    "    #                 'num_blocks': 3,\n",
    "    #                 'base_units': 384,\n",
    "    #                 'dropout_rate': 0.3,\n",
    "    #                 'learning_rate': 5e-4,\n",
    "    #                 'batch_size': 2048,\n",
    "    #                 'epochs': 200\n",
    "    #             }\n",
    "    #         },\n",
    "    #         {\n",
    "    #             'name': 'ResNet_Style_Architecture_3',\n",
    "    #             'params': {\n",
    "    #                 'num_blocks': 4,\n",
    "    #                 'base_units': 512,\n",
    "    #                 'dropout_rate': 0.4,\n",
    "    #                 'learning_rate': 1e-4,\n",
    "    #                 'batch_size': 4096,\n",
    "    #                 'epochs': 200\n",
    "    #             }\n",
    "    #         }\n",
    "    #     ]\n",
    "    # },\n",
    "    \n",
    "    'Deep': {\n",
    "        'network_type': 'wide_and_deep',\n",
    "        'architectures': [\n",
    "            {\n",
    "                'name': 'Deep_Architecture_1',\n",
    "                'params': {\n",
    "                    'deep_layers': [256, 128],\n",
    "                    'dropout_rate': 0.3,\n",
    "                    'learning_rate': 1e-3,\n",
    "                    'batch_size': 2048,\n",
    "                    'epochs': 200\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Deep_Architecture_2',\n",
    "                'params': {\n",
    "                    'deep_layers': [512, 256, 128],\n",
    "                    'dropout_rate': 0.3,\n",
    "                    'learning_rate': 5e-4,\n",
    "                    'batch_size': 2048,\n",
    "                    'epochs': 200\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Deep_Architecture_3',\n",
    "                'params': {\n",
    "                    'deep_layers': [1024, 512, 256, 128],\n",
    "                    'dropout_rate': 0.4,\n",
    "                    'learning_rate': 1e-4,\n",
    "                    'batch_size': 4096,\n",
    "                    'epochs': 200\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "active_networks = list(network_configs.keys())\n",
    "if len(active_networks) != 1:\n",
    "    raise ValueError(f\"ERROR: {len(active_networks)} networks active, need exactly 1\")\n",
    "\n",
    "print(f\"\\nActive: {active_networks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee48d3",
   "metadata": {},
   "source": [
    "# 9. TRAIN ARCHITECTURES WITH ACCUMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for network_name, config in network_configs.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"NETWORK: {network_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    network_type = config['network_type']\n",
    "    architecture_results = []\n",
    "    trained_models = []\n",
    "    \n",
    "    for arch_idx, architecture in enumerate(config['architectures'], 1):\n",
    "        arch_name = architecture['name']\n",
    "        arch_params = architecture['params']\n",
    "        \n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        print(f\"[{arch_idx}/3] {arch_name}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        \n",
    "        batch_size = arch_params['batch_size']\n",
    "        train_ds, val_ds = build_tf_datasets(\n",
    "            X_cat_subtrain, X_num_subtrain, y_subtrain,\n",
    "            X_cat_val, X_num_val, y_val,\n",
    "            predictor_cat_cols,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        print(f\"Building {network_type}...\")\n",
    "        \n",
    "        if network_type == 'resnet_style':\n",
    "            model = build_resnet_style(\n",
    "                embedding_info, num_num_feats, predictor_cat_cols,\n",
    "                num_blocks=arch_params['num_blocks'],\n",
    "                base_units=arch_params['base_units'],\n",
    "                dropout_rate=arch_params['dropout_rate'],\n",
    "                learning_rate=arch_params['learning_rate']\n",
    "            )\n",
    "        elif network_type == 'wide_and_deep':\n",
    "            model = build_wide_and_deep(\n",
    "                embedding_info, num_num_feats, predictor_cat_cols,\n",
    "                deep_layers=arch_params['deep_layers'],\n",
    "                dropout_rate=arch_params['dropout_rate'],\n",
    "                learning_rate=arch_params['learning_rate']\n",
    "            )\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1)\n",
    "        ]\n",
    "        \n",
    "        print(f\"  Training up to {arch_params['epochs']} epochs...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=arch_params['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "        \n",
    "        X_train_inputs = [X_train_cat[col].astype('int32') for col in predictor_cat_cols]\n",
    "        X_train_inputs.append(X_train_num.astype('float32'))\n",
    "        X_train_inputs = tuple(X_train_inputs)\n",
    "        \n",
    "        X_test_inputs = [X_test_cat[col].astype('int32') for col in predictor_cat_cols]\n",
    "        X_test_inputs.append(X_test_num.astype('float32'))\n",
    "        X_test_inputs = tuple(X_test_inputs)\n",
    "        \n",
    "        print(f\"  Calculating metrics...\")\n",
    "        metrics, y_pred_test, y_prob_test = calculate_nn_comprehensive_metrics(\n",
    "            model, network_name, arch_name,\n",
    "            X_train_inputs, X_test_inputs,\n",
    "            y_train, y_test,\n",
    "            training_time,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"\\nRESULTS:\")\n",
    "        print(f\"Training time: {metrics['Training_Time_Minutes']:.2f} min \\n\")\n",
    "\n",
    "        print(f\"Train Precision: {metrics['Train_Precision']:.4f}\")\n",
    "        print(f\"Train Recall: {metrics['Train_Recall']:.4f}\")\n",
    "        print(f\"Train Accuracy: {metrics['Train_Accuracy']:.4f}\")\n",
    "        print(f\"Train F1: {metrics['Train_F1']:.4f}\")\n",
    "        print(f\"Train MCC: {metrics['Train_MCC']:.4f}\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Test Precision: {metrics['Test_Precision']:.4f}\")\n",
    "        print(f\"Test Recall: {metrics['Test_Recall']:.4f}\")\n",
    "        print(f\"Test Accuracy: {metrics['Test_Accuracy']:.4f}\")\n",
    "        print(f\"Test F1: {metrics['Test_F1']:.4f}\")\n",
    "        print(f\"Test MCC: {metrics['Test_MCC']:.4f}\")\n",
    "        print(f\"Test ROC AUC: {metrics['Test_ROC_AUC']:.4f}\\n\")\n",
    "        \n",
    "        architecture_results.append(metrics)\n",
    "        trained_models.append({\n",
    "            'architecture': arch_name,\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'y_pred_test': y_pred_test,\n",
    "            'y_prob_test': y_prob_test,\n",
    "            'history': history\n",
    "        })\n",
    "        \n",
    "        # Save individual\n",
    "        pd.DataFrame([metrics]).to_excel(\n",
    "            f'db/02b_neural_networks/metrics/by_architecture/{network_name}_{arch_name}_comprehensive_metrics.xlsx', \n",
    "            index=False, engine='openpyxl')\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        pd.DataFrame(cm, index=['True_Others', 'True_Displacement'],\n",
    "                     columns=['Pred_Others', 'Pred_Displacement']).to_excel(\n",
    "            f'db/02b_neural_networks/model_data/confusion_matrices/by_architecture/{network_name}_{arch_name}_confusion_matrix.xlsx', \n",
    "            engine='openpyxl')\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)\n",
    "        pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds, 'AUC': auc(fpr, tpr)}).to_excel(\n",
    "            f'db/02b_neural_networks/model_data/roc_data/by_architecture/{network_name}_{arch_name}_roc_curve.xlsx', \n",
    "            index=False, engine='openpyxl')\n",
    "        \n",
    "        report = classification_report(y_test, y_pred_test, target_names=['Others', 'Displacement'], output_dict=True)\n",
    "        pd.DataFrame(report).transpose().to_excel(\n",
    "            f'db/02b_neural_networks/metrics/by_architecture/{network_name}_{arch_name}_classification_report.xlsx', \n",
    "            engine='openpyxl')\n",
    "        \n",
    "        pd.DataFrame(history.history).to_excel(\n",
    "            f'db/02b_neural_networks/model_data/learning_curves/by_architecture/{network_name}_{arch_name}_learning_curves.xlsx', \n",
    "            index=False, engine='openpyxl')\n",
    "        \n",
    "        pd.DataFrame([{'network_type': network_type, **arch_params}]).to_excel(\n",
    "            f'db/02b_neural_networks/model_data/hyperparameters/{network_name}_{arch_name}_hyperparameters.xlsx', \n",
    "            index=False, engine='openpyxl')\n",
    "        \n",
    "        print(f\"Saved\")\n",
    "    \n",
    "    # Compare\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPARING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame(architecture_results).sort_values('Test_F1', ascending=False)\n",
    "    comparison_df.to_excel(\n",
    "        f'db/02b_neural_networks/model_data/architecture_comparisons/{network_name}_architectures_comparison.xlsx', \n",
    "        index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"\\nRanked:\")\n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        print(f\"  {idx+1}. {row['Architecture']}: F1={row['Test_F1']:.4f}\")\n",
    "    \n",
    "    best_arch_name = comparison_df.iloc[0]['Architecture']\n",
    "    print(f\"\\nBEST: {best_arch_name}\")\n",
    "    \n",
    "    best_model_data = None\n",
    "    for trained_data in trained_models:\n",
    "        if trained_data['architecture'] == best_arch_name:\n",
    "            best_model_data = trained_data\n",
    "            break\n",
    "    \n",
    "    # Save best\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAVING BEST\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model_data['model'].save(f'db/02b_neural_networks/saved_models/{network_name}_best_model.keras')\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'Sample_Index': range(len(y_test)),\n",
    "        'True_Label': y_test,\n",
    "        'Predicted_Label': best_model_data['y_pred_test'],\n",
    "        'Probability_Class_1': best_model_data['y_prob_test'],\n",
    "        'Correct': y_test == best_model_data['y_pred_test']\n",
    "    }).to_csv(f'db/02b_neural_networks/predictions/{network_name}_best_predictions.csv', index=False)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, best_model_data['y_pred_test'])\n",
    "    pd.DataFrame(cm, index=['True_Others', 'True_Displacement'],\n",
    "                 columns=['Pred_Others', 'Pred_Displacement']).to_excel(\n",
    "        f'db/02b_neural_networks/model_data/confusion_matrices/{network_name}_best_confusion_matrix.xlsx', \n",
    "        engine='openpyxl')\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, best_model_data['y_prob_test'])\n",
    "    pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds, 'AUC': auc(fpr, tpr)}).to_excel(\n",
    "        f'db/02b_neural_networks/model_data/roc_data/{network_name}_best_roc_curve.xlsx', \n",
    "        index=False, engine='openpyxl')\n",
    "    \n",
    "    pd.DataFrame([best_model_data['metrics']]).to_excel(\n",
    "        f'db/02b_neural_networks/metrics/{network_name}_best_comprehensive_metrics.xlsx', \n",
    "        index=False, engine='openpyxl')\n",
    "    \n",
    "    report = classification_report(y_test, best_model_data['y_pred_test'], \n",
    "                                   target_names=['Others', 'Displacement'], output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_excel(\n",
    "        f'db/02b_neural_networks/metrics/{network_name}_best_classification_report.xlsx', \n",
    "        engine='openpyxl')\n",
    "    \n",
    "    pd.DataFrame(best_model_data['history'].history).to_excel(\n",
    "        f'db/02b_neural_networks/model_data/learning_curves/{network_name}_best_learning_curves.xlsx', \n",
    "        index=False, engine='openpyxl')\n",
    "    \n",
    "    best_params = None\n",
    "    for arch in config['architectures']:\n",
    "        if arch['name'] == best_arch_name:\n",
    "            best_params = {'network_type': network_type, **arch['params']}\n",
    "            break\n",
    "    if best_params:\n",
    "        pd.DataFrame([best_params]).to_excel(\n",
    "            f'db/02b_neural_networks/model_data/hyperparameters/{network_name}_best_hyperparameters.xlsx', \n",
    "            index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"Best saved\")\n",
    "    \n",
    "    # ACCUMULATION\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ACCUMULATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # All architectures\n",
    "    all_arch_path = 'db/02b_neural_networks/metrics/all_architectures_tested_comprehensive.xlsx'\n",
    "    new_arch_results = pd.DataFrame(architecture_results)\n",
    "    \n",
    "    if os.path.exists(all_arch_path):\n",
    "        print(f\"Loading previous...\")\n",
    "        previous_arch = pd.read_excel(all_arch_path)\n",
    "        previous_arch = previous_arch[previous_arch['Model'] != network_name]\n",
    "        all_arch_combined = pd.concat([previous_arch, new_arch_results], ignore_index=True)\n",
    "        print(f\"Previous: {len(previous_arch)} | New: {len(new_arch_results)} | Total: {len(all_arch_combined)}\")\n",
    "    else:\n",
    "        print(f\"Creating new...\")\n",
    "        all_arch_combined = new_arch_results\n",
    "        print(f\"Total: {len(all_arch_combined)}\")\n",
    "    \n",
    "    all_arch_combined.to_excel(all_arch_path, index=False, engine='openpyxl')\n",
    "    print(f\"Saved\")\n",
    "    \n",
    "    # Best models\n",
    "    best_models_path = 'db/02b_neural_networks/comparative_tables/best_models_comparison_complete.xlsx'\n",
    "    new_best_model = pd.DataFrame([best_model_data['metrics']])\n",
    "    \n",
    "    if os.path.exists(best_models_path):\n",
    "        print(f\"Loading previous best...\")\n",
    "        previous_best = pd.read_excel(best_models_path)\n",
    "        previous_best = previous_best[previous_best['Model'] != network_name]\n",
    "        best_combined = pd.concat([previous_best, new_best_model], ignore_index=True)\n",
    "        best_combined = best_combined.sort_values('Test_F1', ascending=False).reset_index(drop=True)\n",
    "        print(f\"Previous: {len(previous_best)} | New: 1 | Total: {len(best_combined)}\")\n",
    "    else:\n",
    "        print(f\"Creating new...\")\n",
    "        best_combined = new_best_model\n",
    "        print(f\"Total: {len(best_combined)}\")\n",
    "    \n",
    "    best_combined.to_excel(best_models_path, index=False, engine='openpyxl')\n",
    "    print(f\"Saved\")\n",
    "    \n",
    "    # Publication-ready\n",
    "    pub_cols = [\n",
    "        'Model', 'Architecture', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1',\n",
    "        'Test_Specificity', 'Test_G_Mean', 'Test_MCC', 'Test_Balanced_Accuracy',\n",
    "        'Test_ROC_AUC', 'Test_Kappa', 'Test_Log_Loss',\n",
    "        'Training_Time_Minutes', 'Inference_Time_ms_per_sample',\n",
    "        'Model_Size_MB', 'Total_Parameters', 'F1_Gap_Train_Test'\n",
    "    ]\n",
    "    pub_table = best_combined[pub_cols].copy()\n",
    "    numeric_cols = pub_table.select_dtypes(include=[np.number]).columns\n",
    "    pub_table[numeric_cols] = pub_table[numeric_cols].round(4)\n",
    "    pub_table.to_excel('db/02b_neural_networks/comparative_tables/best_models_comparison_publication_ready.xlsx', \n",
    "                       index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"\\nAccumulation completed\")\n",
    "    print(f\"Network added: {network_name}\")\n",
    "    print(f\"Total networks: {len(best_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3db77",
   "metadata": {},
   "source": [
    "# 10. TRAINING COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe32c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_path = 'db/02b_neural_networks/comparative_tables/best_models_comparison_complete.xlsx'\n",
    "if os.path.exists(best_models_path):\n",
    "    current_best = pd.read_excel(best_models_path).sort_values('Test_F1', ascending=False)\n",
    "    \n",
    "    print(f\"\\nCURRENT STATE:\")\n",
    "    print(f\"Total networks: {len(current_best)}\")\n",
    "    print(f\"\\nTop 3:\")\n",
    "    for idx, row in current_best.head(3).iterrows():\n",
    "        print(f\"    {idx+1}. {row['Model']}: F1={row['Test_F1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nAll trained:\")\n",
    "    for network in current_best['Model'].tolist():\n",
    "        print(f\"{network}\")\n",
    "else:\n",
    "    print(\"\\nNo networks found\")\n",
    "\n",
    "print(\"NEXT: Comment current network, uncomment next, run again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e79185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 2.1.3\n",
      "tensorflow: 2.20.0\n",
      "sklearn: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow\n",
    "import sklearn\n",
    "print(f\"numpy: {numpy.__version__}\")\n",
    "print(f\"tensorflow: {tensorflow.__version__}\")\n",
    "print(f\"sklearn: {sklearn.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simulations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
